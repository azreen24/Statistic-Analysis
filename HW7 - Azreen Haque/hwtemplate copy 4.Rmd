---
output: 
  pdf_document: 
    latex_engine: xelatex
header-includes:
  - \usepackage{amsmath}
  - \usepackage{array}
geometry: margin=1in
---

**Azreen Haque** 

**4/11/2025**

**Solutions**

## Problem 1
Solutions below.

### Part A
```{r}
data <- read.csv("hw07pr01.csv", header = TRUE, sep = ",")

# Fit linear model
model1 <- lm(steroid ~ age, data = data)

# Get summary and store it in 's'
s <- summary(model1)

# Extract regression components
intercept <- s$coefficients["(Intercept)", "Estimate"]
slope <- s$coefficients["age", "Estimate"]
r_squared <- s$r.squared
adj_r_squared <- s$adj.r.squared
p_value_slope <- s$coefficients["age", "Pr(>|t|)"]

# Print all extracted values
cat("Fitted equation: Ŷ =", round(intercept, 5), "+", round(slope, 5), "* age\n")
cat("R-squared:", round(r_squared, 4), "\n")
cat("Adjusted R-squared:", round(adj_r_squared, 4), "\n")
cat("P-value for age coefficient:", format.pval(p_value_slope, digits = 3), "\n")
cat(
  "The p-value for the slope coefficient (β1) is less than 2e-16,\n",
  "which is extremely small. This provides strong evidence against\n",
  "the null hypothesis H0: β1 = 0.\n\n",
  "We conclude that age is a statistically significant predictor\n",
  "of steroid level in this sample of 75 individuals.\n"
)
```

### Part B
```{r}
# Create residual plot
plot(data$age, resid(model1), 
     main = "Residual Plot", 
     xlab = "Age", 
     ylab = "Residuals")
abline(h = 0, lty = 2)

# Create standardized residual plot
sse <- sum(resid(model1)^2)
n <- nrow(data)
mse <- sse / (n - 2)
standardized_res <- resid(model1) / sqrt(mse)

plot(data$age, standardized_res,
     main = "Standardized Residual Plot",
     xlab = "Age",
     ylab = "Standardized Residuals",
     ylim = c(-3.5, 3.5))
abline(h = 0, lty = 2)
abline(h = c(-3, 3), lty = 3)

cat(
  "The residual plot for the linear model shows a U-shaped pattern,\n",
  "suggesting that the relationship between age and steroid level\n",
  "is not adequately captured by a simple linear model.\n\n",

  "This is a violation of the linearity assumption.\n\n",

  "The variance of residuals appears roughly constant,\n",
  "and no standardized residuals exceed ±3,\n",
  "so the constant variance and outlier assumptions\n",
  "are reasonably met.\n\n",

  "Conclusion: The linear model violates the linearity assumption,\n",
  "and a more appropriate model may be quadratic."
)
```

### Part C
```{r}
# Create quadratic term
data$age2 <- data$age^2

# Fit quadratic regression model
model2 <- lm(steroid ~ age + age2, data = data)

# Summarize the model
s2 <- summary(model2)

# Extract coefficients
b0 <- s2$coefficients["(Intercept)", "Estimate"]
b1 <- s2$coefficients["age", "Estimate"]
b2 <- s2$coefficients["age2", "Estimate"]

# Extract R-squared and Adjusted R-squared
r2 <- s2$r.squared
adj_r2 <- s2$adj.r.squared

# Extract p-value for β1 (age)
p_b1 <- s2$coefficients["age", "Pr(>|t|)"]

# Print results
cat("Fitted equation:\n")
cat("Ŷ = ", round(b0, 5), " + ", round(b1, 5), "* age + ", round(b2, 5), "* age²\n\n")
cat("R-squared: ", round(r2, 4), "\n")
cat("Adjusted R-squared: ", round(adj_r2, 4), "\n")
cat("P-value for β1 (age): ", format.pval(p_b1, digits = 3), "\n")

cat(
  "Comparison to Part (a):\n",
  "The quadratic model has a higher R² (0.9796 vs. 0.9498)\n",
  "and higher adjusted R² (0.979 vs. 0.9492),\n",
  "indicating a better fit.\n\n",
  
  "However, the p-value for age is no longer significant (0.112),\n",
  "suggesting age may not be linearly associated with steroid\n",
  "after accounting for the quadratic term.\n"
)
```

### Part D
```{r}
# Residual plot for quadratic model
plot(data$age, resid(model2),
     main = "Residual Plot (Quadratic Model)",
     xlab = "Age",
     ylab = "Residuals")
abline(h = 0, lty = 2)

# Standardized residuals
sse2 <- sum(resid(model2)^2)
n <- nrow(data)
mse2 <- sse2 / (n - 3)  # 3 parameters: intercept, age, age^2
standardized_res2 <- resid(model2) / sqrt(mse2)

# Standardized residual plot
plot(data$age, standardized_res2,
     main = "Standardized Residual Plot (Quadratic)",
     xlab = "Age",
     ylab = "Standardized Residuals",
     ylim = c(-3.5, 3.5))
abline(h = 0, lty = 2)
abline(h = c(-3, 3), lty = 3)

cat(
  "The residual plot for the quadratic model shows\n",
  "no clear pattern or curvature, suggesting that\n",
  "linearity is reasonably addressed.\n\n",

  "The residuals appear to be evenly spread across age,\n",
  "indicating that the constant variance assumption holds.\n\n",

  "All standardized residuals are within ±3, so there\n",
  "are no major outliers or influential points.\n\n",

  "Conclusion: There are no noticeable violations\n",
  "of regression assumptions in the quadratic model."
)
```

### Part E
```{r}
# Compute correlation between X and X^2
cor_age_age2 <- cor(data$age, data$age2)
cat("Correlation between age and age²:", round(cor_age_age2, 4), "\n")
cat(
  "The correlation between age and age² is very close to 1.\n",
  "This indicates strong collinearity between the two predictors.\n\n",
  "When two variables are highly correlated, including both\n",
  "in a regression model may lead to multicollinearity.\n",
  "This can inflate standard errors and make it harder\n",
  "to interpret the individual effect of each predictor."
)
```

### Part F
```{r}
# Center the age variable
data$x <- data$age - mean(data$age)

# Compute x²
data$x2 <- data$x^2

# Correlation between x and x²
cor_x_x2 <- cor(data$x, data$x2)

# Print result and compare
cat("Correlation between x and x²:", round(cor_x_x2, 4), "\n\n")
cat(
  "Correlation between x and x²: 0.1916\n\n",
  "Compared to part (e), the correlation dropped\n",
  "from 0.9891 to 0.1916 after centering age.\n\n",
  "This reduces multicollinearity between\n",
  "the linear and quadratic terms.\n\n"
)
```


### Part G
```{r}
# Fit quadratic model using centered x and x^2
model_centered <- lm(steroid ~ x + x2, data = data)

# Get summary
s_centered <- summary(model_centered)

# Extract coefficients
b0_c <- s_centered$coefficients["(Intercept)", "Estimate"]
b1_c <- s_centered$coefficients["x", "Estimate"]
b2_c <- s_centered$coefficients["x2", "Estimate"]

# Extract R-squared, adjusted R-squared, and p-value for β1
r2_c <- s_centered$r.squared
adj_r2_c <- s_centered$adj.r.squared
pval_b1_c <- s_centered$coefficients["x", "Pr(>|t|)"]

# Print result
cat("Fitted equation:\n")
cat("Ŷ = ", round(b0_c, 5), " + ", round(b1_c, 5), "* x + ", round(b2_c, 5), "* x²\n\n")
cat("R-squared: ", round(r2_c, 4), "\n")
cat("Adjusted R-squared: ", round(adj_r2_c, 4), "\n")
cat("P-value for β1 (x): ", format.pval(pval_b1_c, digits = 3), "\n\n")

cat("Compared to part (c):\n",
  "- R² and adjusted R² remain the same because the model fit\n",
  "  hasn't changed, just the variable scale.\n",
  "- The coefficient for β1 is easier to interpret now,\n",
  "  since x is centered.\n",
  "- The p-value for β1 is now highly significant, showing\n",
  "  improved stability due to reduced multicollinearity.\n"
)
```


## Problem 2

### Part A
```{r}
# Load data
data2 <- read.csv("hw07pr02.csv", header = TRUE)

# Fit regression model with price and discount only
model_a <- lm(market.share ~ price + discount, data = data2)

summary(model_a)

cat(
  "Fitted equation:\n",
  "Ŷ = 2.8684 – 0.1391 * price + 0.2749 * discount\n\n",

  "Interpretation:\n",
  "β₁ (price): For every $1 increase in price, the market share\n",
  "is expected to decrease by 0.1391 units,\n",
  "holding discount constant. (p = 0.470 — not significant)\n\n",

  "β₂ (discount): Discounted products have, on average,\n",
  "0.2749 units higher market share than non-discounted products,\n",
  "holding price constant. (p = 0.0298 — statistically significant)\n\n",

  "Group-specific equations:\n",
  "Non-discounted (discount = 0):\n",
  "Ŷ = 2.8684 – 0.1391 * price\n\n",
  "Discounted (discount = 1):\n",
  "Ŷ = 3.1433 – 0.1391 * price\n"
)
```

### Part B
```{r}
# Fit model with interaction between price and discount
model_b <- lm(market.share ~ price * discount, data = data2)

# Show summary output
summary(model_b)
cat(
  "Fitted equation:\n",
  "Ŷ = -2.1049 + 1.9755 * price + 5.7756 * discount\n",
  "     – 2.3345 * (price × discount)\n\n",

  "Interpretation of β₃ (interaction):\n",
  "The effect of price on market share differs depending\n",
  "on whether the product is discounted. Specifically,\n",
  "the slope for price decreases by 2.3345 units when the\n",
  "product is discounted. (p < 0.001 — highly significant)\n\n",

  "Group-specific fitted equations:\n",
  "Non-discounted (discount = 0):\n",
  "Ŷ = -2.1049 + 1.9755 * price\n\n",
  "Discounted (discount = 1):\n",
  "Ŷ = (-2.1049 + 5.7756) + (1.9755 – 2.3345) * price\n",
  "Ŷ = 3.6707 – 0.3590 * price\n"
)

```

### Part C
```{r}
# Fit the full model with all predictors
model_c <- lm(market.share ~ price + discount + promotion + rating, data = data2)

# Show fitted equation
summary(model_c)

cat(
  "Fitted equation:\n",
  "Ŷ = 2.7262 – 0.2361 * price + 0.2887 * discount\n",
  "     + 0.1482 * promotion + 0.0006831 * rating\n"
)
```

### Part D
```{r}
# Load leaps package
library(leaps)

# Run subset selection
subset_model <- regsubsets(market.share ~ price + discount + promotion + rating,
                           data = data2, nbest = 3)

# Display summary
subset_summary <- summary(subset_model)

# Show results
print(data.frame(
  Variables = apply(subset_summary$which, 1, function(x) paste(names(x)[x][-1], collapse = ", ")),
  R2 = round(subset_summary$rsq, 4),
  Adj_R2 = round(subset_summary$adjr2, 4),
  Cp = round(subset_summary$cp, 4)
))

cat(
  "Model selection summary:\n\n",
  
  "Best model by Adjusted R-squared:\n",
  "  Variables: price, discount, rating\n",
  "  Adjusted R² = 0.2074\n\n",
  
  "Best model by Mallows' Cp:\n",
  "  Variables: discount, rating\n",
  "  Cp = 3.7385 (closest to p + 1 = 3)\n\n",
  
  "Conclusion:\n",
  "  The model with discount and rating has the most favorable Cp,\n",
  "  indicating minimal bias and good fit.\n",
  "  However, the model with price, discount, and rating has the highest\n",
  "  adjusted R² and may explain more variance.\n"
)
```

### Part E
```{r}
cat(
  "Total models checked in subset selection:\n",
  "For 4 predictors, all possible non-empty subsets = 15\n"
)
```

### Part F
```{r}
model_f <- lm(market.share ~ price + discount + rating, data = data2)
summary(model_f)
anova(model_f)

# Manually confirm stats for: model_f = price + discount + rating
model_f <- lm(market.share ~ price + discount + rating, data = data2)
model_full <- lm(market.share ~ price + discount + promotion + rating, data = data2)

# Sample size and number of predictors
n <- nrow(data2)
p <- 3   # number of predictors in model_f
p_full <- 4

# Extract RSS (Residual Sum of Squares)
rss <- sum(resid(model_f)^2)

# Total Sum of Squares (SST)
sst <- sum((data2$market.share - mean(data2$market.share))^2)

# R-squared
r_squared <- 1 - rss / sst

# Adjusted R-squared
adj_r_squared <- 1 - (rss / (n - p - 1)) / (sst / (n - 1))

# Get MSE from full model
mse_full <- summary(model_full)$sigma^2  # sigma is residual std error

# Mallows' Cp
cp <- rss / mse_full - (n - 2 * p)

# Show results
cat("Manual calculations:\n")
cat("R-squared:         ", round(r_squared, 4), "\n")
cat("Adjusted R-squared:", round(adj_r_squared, 4), "\n")
cat("Mallows' Cp:       ", round(cp, 4), "\n")

```

